## Several Transformer network variants tutorials

#### 1. transformer_encoder, [src](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
    * Use Pytorch nn.transformer package to build an encoder for language prediction
   
#### 2. transformer_xl_from_scratch, [src](https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/)
    * Build Transformer + XLNet
    
#### 3. transformer_xl_multihead, [src](https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/)
    * Build Transformer + XLNet + MultiAttention heads

#### 4. xlnet, [src](https://github.com/graykode/xlnet-Pytorch)
    * Build standard [XLNet](https://arxiv.org/pdf/1906.08237.pdf)
    * More tutorial [1](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)

#### TODO. More detailed code about xlnet
    
#### TODO. ALBERT, [paper](https://arxiv.org/abs/1909.11942v1)
    * Lite BERT
